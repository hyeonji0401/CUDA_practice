{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwewF8ec6ATUIt4TjDnalX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonji0401/CUDA_practice/blob/main/CUDA_execution_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1 엔디비아 GPU 아키텍처\n",
        "- CUDA 지원 엔비디아 GPU 아키텍처는 2008년부터 발표\n",
        "\n",
        "- 페르미 아키텍처 (2010년)\n",
        " - CUDA 관점에서 구조가 정립된 첫번째 아키텍처\n",
        " - SM 안에 같은 크기의 정사각형 하나하나가 일종의 연산 코어이며 이는 SM으로 묶임\n",
        "\n",
        "- 암페어 아키텍처 (2020년)\n",
        " - 스트리밍 멀티프로세서(SM)는 수많은 CUDA코어로 이루어져있으며, 코어가 모여서 그룹을 이룸\n",
        " - 캐시와 메모리 제어기도 확인 가능함\n",
        "\n",
        "\n",
        "**1.스트리밍 멀티프로세서(SM:streaming processor)**\n",
        "- 하나의 GPU는 여러 개의 SM을 포함\n",
        "- 여러 개의 CUDA 코어를 가진 연산장치\n",
        "- 아키텍처마다 SM이 포함하는 CUDA 코어 수는 다름\n",
        " - 페르미 아키텍처의 경우 하나의 SM에 32개의 CUDA 코어를 가지고 있음\n",
        "- CUDA 코어 외에도 레지스터 파일, 공유 메모리, L1 캐시 등이 포함됨\n",
        "\n",
        "\n",
        "**2. CUDA 코어**\n",
        "- GPU의 가장 기본적인 프로세싱 유닛(processing unit)으로 연산을 수행하는 가장 기본 단위\n",
        "- 내부에 FP unit, INT unit과 같이 실제 연산 유닛을 확인할 수 있음\n",
        "- CUDA 프로그램의 기본 동작 단위는 스레드이므로 CUDA 코어 하나가 스레드 하나를 처리한다고 말할 수 있음\n",
        "- GPU 사양에서 말하는 코어 수는 일반적으로 CUDA 코어 수를 의미함"
      ],
      "metadata": {
        "id": "dzHZEpIVh090"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2 CUDA 스레드 계층과 GPU 하드웨어\n",
        "- CUDA 스레드 계층\n",
        " - 32개 스레드 -> 워프 -> 스레드 블록 -> 그리드\n",
        "\n",
        "**1.그리드 -> GPU**\n",
        "- CUDA 커널 수행 시 스레드 레이아웃에 따라 그리드 생성\n",
        "- 그리드는 GPU를 사용하는 단위, 즉, 하나의 그리드는 한 GPU에서 실행됨\n",
        " - 여러 개의 GPU를 가진 멀티 GPU시스템이어도 하나의 그리드가 여러 개의 GPU를 동시에 사용하거나 GPU를 옮겨가며 실행될 수 없음\n",
        " - 하나의 GPU가 여러개의 그리드를 처리할 수는 있음\n",
        "\n",
        " => 하나의 GPU를 이용해서 여러 개의 CUDA 프로그램을 동시에 실행 가능\n",
        "\n",
        "**2.스레드 블록 -> SM**\n",
        "- 그리드에 포함된 스레드 블록은 그리드가 배정된 GPU 속 SM에 의해 처리됨\n",
        "\n",
        "=> 스레드 블록을 처리하는 단위는 SM\n",
        "\n",
        "=> 그리드에 속한 스레드 블록 개수는 스레드 레이아웃에 따라 달라짐\n",
        "\n",
        "- 스레드 블록은 SM에 순차적으로 균등하게 분배되어 처리됨\n",
        "- 하나의 SM에 여러 개의 블록이 할당될 수도 있음\n",
        " - SM이 갖는 자원의 양과 한 스레드 블록을 처리하기 위해 필요한 자원(레지스터, 공유 메모리 등과 같은 메모리 자원)의 양에 따라 한 SM이 동시에 처리할 수있는 스레드 블록 수 가 결정됨\n",
        "- 활성 블록 : SM에 할당된 블록 중 현재 필요한 자원을 모두 할당받고 실행할 수 있는 스레드 블록\n",
        " - 활성 블록의 수는 CUDA 프로그램의 성능에 큰 영향을 주며 블록당 사용 자원 양을 조절함으로써 활성 블록 수를 증가시키는 전략을 사용하기도 함\n",
        "\n",
        "**3.워프&스레드 -> SM 속의 CUDA 코어**\n",
        "- 스레드 블록에 포함된 스레드들은 워프로 분할됨\n",
        "- 워프는 32개의 스레드로 구성되어있으며 각각 CUDA 코어 하나에서 처리됨\n",
        " - 페르미 아키텍처의 경우 SM 내 CUDA 코어의 수가 32이며 16씩 두 개의 그룹으로 분리되어있음\n",
        " - 각 CUDA 코어 그룹이 하나의 워프를 처리함\n",
        " - SM 내부의 CUDA 코어 개수는 아키텍처에 따라 다르지만, 대체로 32의 배수임(워프가 32개의 스레드로 구성되었기 때문)\n",
        "- 워프 속 스레드들은 하나의 명령어로 제어되며 GPU가 SIMT 아키텍처라는 말은 이에 의해 나옴\n",
        "- 워프 스케줄러(warp scheduler)와 명령어 전달 유닛(dispatch unit)들이 다음에 처리할 워프를 결정하거나 명령을 내리고 각 CUDA 코어는 명령어에 따라 스레드의 작업을 처리\n",
        "\n",
        "**스레드의 실행 문맥**\n",
        "- 워프 내 스레드들은 하나의 명령어에 의해 움직이지만 각 스레드는 독립적으로 처리될 수도 있음\n",
        " - 스레드는 자신만의 실행문맥(execution context)을 가짐\n",
        "\n",
        " => GPU는 이러한 관점에서 SIMT라고 할 수 있음\n",
        " - SIMD : 하나의 실행 문맥 또는 하나의 스레드에서 하나의 명령어로 여러 개의 데이터를 처리\n",
        " - SIMT : 자신만의 실행 문맥을 가진 스레드가 하나의 명령어에 의해 제어됨\n",
        "- 실행 문맥 : 작업 상태에 대한 기록\n",
        " - GPU의 각 스레드는 자신만의 작업 상황을 레지스터에 저장하고 있음\n",
        " - 스레드 블록 내 모든 워프가 SM 내부 레지스터 파일(레이스터들의 집합)을 나누어서 사용함\n",
        "\n",
        " ex) 스레드 블록 안의 스레드 개수 = 512개\n",
        "\n",
        "     레지스터 파일 내 레지스터 개수 = 5,120개\n",
        "\n",
        "     각 스레드는 10개의 레지스터를 사용하는 형태가 됨\n",
        "\n",
        "=> 이러한 실행 구조는 워프 처리에 대해 무비용 문맥 교환과 워프 분기라는 두가지 특성을 만들어냄\n",
        "\n",
        "**4.무비용 문맥 교환(Zero Context Switch Overhead)**\n",
        "- 문맥 교환 : 연산 장치를 사용하는 프로세스가 교체될 때 먼저 연산 장치를 사용 중이던 프로세스의 문맥을 메모리 저장(context saving)하고, 저장되어 있던 새로 들어오는 프로세스의 문맥을 메모리에서 가져와 복원(context restoring)하는 과정을 말함\n",
        " - **CPU의 경우(교재 141pg 그림 6-7참조)**\n",
        "   - p out : 코어를 사용 중인 프로세스\n",
        "   - p in : 다음 차례 프로세스\n",
        "   - p out 이 코어를 넘겨주어야하면 작업은 종료되어야함\n",
        "   - p out 이 다시 코어를 할당받았을 때 이전 상황에 대해 알고 있어야함\n",
        "   - 따라서 p out이 코어를 빼앗길 때 현재 문맥을 메모리에 저장함(문맥 저장: context saving)\n",
        "   - p in 의 경우는 기존에 저장해두었던 작업 상황을 복원하고 작업을 재개함(문맥 복원: context restoring)\n",
        "\n",
        "     => 문맥 교환(context switch) : 프로세스 교체 과정에서 발생하는 문맥 저장과 문맥 복원 과정을 묶어서 지칭하는 것\n",
        "\n",
        " - 문맥은 일반적으로 프로세서 또는 코어 안에 있는 레지스터 값을 말함\n",
        " - 문맥 쿄환은 많은 비용(overhead)을 요구하며 잦은 문맥 교환은 시스템의 성능을 크게 낮추기도 함\n",
        "\n",
        " => 일반적인 시스템에서는 잦은 문맥 교환을 되도록 방지하는 것이 좋음\n",
        "\n",
        " => 그러나 GPU알고리즘의 경우 코어 수보다 적게는 3~4배에서 많게는 10배 이상의 스레드를 사용하길 권장\n",
        "\n",
        " => GPU는 문맥 교환 비용이 0에 가까움\n",
        "\n",
        " - **GPU의 경우 (교재 143pg 그림 6-8참조)**\n",
        "   - SM 안에는 여러 개의 워프가 있고 워프는 서로 번갈아 CUDA 코어를 사용\n",
        "   - 워프가 전환 되는 과정(CUDA 코어를 사용하는 스레드 교체)에서 그에 따라 CUDA 코어가 참조하는 문맥도 바뀌어야함\n",
        "  \n",
        "   => 문맥 교환 발생\n",
        "\n",
        "   - 다른 연산 장치와 달리 블록 내 스레드는 SM 레지스터 파일을 나누어 사용하기 때문에 메모리에 복사하거나 메모리에서 읽어오는 작업이 불필요함\n",
        "\n",
        "   - 스레드가 잠시 일을 멈추더라도 스레드의 문맥(레지스터 값)은 유지되며 다시 CUDA 코어를 할당받으면 다른 작업 없이 바로 연산을 시작할 수 있음\n",
        "\n",
        "   => 문맥 교환 부하 없음\n",
        "\n",
        "=> 스레드가 메모리로부터 데이터를 읽어오는 과정에서 발생하는 연산 지연(latency)을 숨기기 위해 많은 수의 스레드를 사용하는 전략에서 활용됨"
      ],
      "metadata": {
        "id": "VhcktxFEfuSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.워프 분기(warp divergence)**\n",
        "- GPU에서 스레드는 자신만의 실행문맥을 가지므로 독립적으로 실행되어도 문제없는 것으로 생각되지만 워프 내 스레드들은 같은 명령어로 움직인다는 점에서 스레드는 독립적으로 움직일 수 없는 것으로 보임\n",
        "\n",
        "**분기(if)가 있는 커널에서 워프 내 스레드들이 서로 다른 길을 가야한다면?**\n",
        "\n",
        "분기가 있는 커널 코드의 예\n",
        "```\n",
        "__global__ void kernel_with_branch(int *_output)\n",
        "{\n",
        "  if(threadIdx.x % 2 == 0)\n",
        "    _output[threadIdx.x] = 1;\n",
        "  else\n",
        "   _output[threadIdx.x] = 0;\n",
        "}\n",
        "```\n",
        "- x-차원 번호가 홀수인 스레드와 짝수인 스레드가 서로 다른 명령어를 수행해야함\n",
        "- 그러나 GPU는 워프에게 한번에 하나의 명령어만 지시할 수 있음\n",
        "\n",
        "=> 각 분기를 순차적으로 처리하는 방법으로 문제 해결\n",
        "\n",
        "- 한 쪽 분기를 처리하는 동안 다른 분기에 속하는 스레드들은 아무것도 하지 않아 이 스레드들 담당하는 CUDA 코어는 아무 작업을 하지않아 사용가능한 연산 자원이 낭비됨\n",
        "\n",
        "=> **워프 분기** : 한 워프 내 스레드들이 서로 다른 분기를 선택하여 작업이 직렬화되어 실행되는 현상\n",
        "\n",
        "- 분기에 2가지 경로가 있을 시 없을 때보다 대비 연산 두 배 느려짐\n",
        "- 워프 내 스레드는 32개이며 최악의 경우 32개의 스레드가 모두 다른 분기를 따를 경우 최대 32배까지 연산 속도가 느려질 수 있음\n",
        "\n",
        "=> 워프 분기는 CUDA 프로그램의 성능을 크게 떨어뜨릴 수 있으므로 피하는 것이 좋음"
      ],
      "metadata": {
        "id": "LYJ-O5WIDyGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 메모리 접근 대기 시간 숨기기 전략\n",
        "- 일반적인 데이터 처리\n",
        " - 메모리 접근(memeory access) + 연산(computation)을 반복하는 구조\n",
        " - 메모리에 접근하는 동안 연산장치는 아무 작업을 하지 못하고 쉬게 됨\n",
        "\n",
        " => 메모리 접근 대기 시간(memory access latency) / 메모리 접근 지연 시간 발생\n",
        "- GPU는 수만 개의 코어를 가지므로 메모리 접근 대기 시간을 최소화해야 효율을 최대화할 수 있음\n",
        " - GPU는 CPU에 비해 코어 수가 확연히 많으면 메모리 타입 또한 최신 메모리(높은 대역폭)를 사용함\n",
        "\n",
        " => 많은 수의 연산 코어에 데이터를 빠르게 공급하기 위한 전략\n",
        "\n",
        " => trade off로 CPU대비 메모리 크기는 작음\n",
        "- 메모리 접근 대기 시간 방지를 위한 CUDA의 전략\n",
        " - 하드 웨어 : 고대역폭 메모리 사용\n",
        " - 소프트웨어 : CUDA 코어 수보다 많은 수의 스레드를 사용\n",
        "  - 한 스레드가 메모리 대기하는 동안 다른 스레드가 CUDA 코어를 사용하게하여 CUDA 코어가 쉬지 않고 일할 수 있게 함\n",
        "\n",
        "  => 접근 대기 시간 숨기기(latency hiding) 전략\n",
        "- GPU에서 메모리 접근 대기 시간 숨기기 전략을 사용할 수 있는 이유는 '무비용 문맥 교환' 특성 때문\n",
        " - 본 전략은 스레드 사이의 잦은 전환이 필요하여 문맥 교환 비용 교환이 크다면 소용없는 전략임\n",
        "- 알고리즘에 따라 연산, 메모리 접근 시간의 비중 및 상호 전환 빈도는 다를 수 있음\n",
        " - 데이터 접근 잦은(IO-bounded) 알고리즘이라면 사용하는 스레드 수를 늘리는 것이 효율적임(복잡도 대비 데이터 접근 횟수가 잦기 때문)\n",
        " - 계산 집약적(compute-bounded) 알고리즘이라면 너무 많은 스레드를 사용하면 성능에 악영향을 줄 수 있음\n",
        "\n",
        " => GPU가 갖는 CUDA 코어 수의 10배 내외를 기준으로 코드를 작성한 후 정상 작동 확인 후 스레드 수 등을 튜닝하는 것을 추천"
      ],
      "metadata": {
        "id": "91wsiGm1kLwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.4 GPU 정보 확인하기\n",
        "- cudaDeviceProp : CUDA에서 GPU 속성 값을 담기 위해 사용하는 구조체 자료형\n",
        " - name : GPU이름\n",
        " - major : compute capability 주 버전\n",
        " - minor : compute capability 부 버전\n",
        " - multiprocessorCount : GPU가 가진 멀티프로세서\n",
        " - totlaGlobalMem : GPU의 global 메모리 크기\n",
        "\n",
        "- GPU의 정보를 얻어서 cudaDeviceProp 구조체에 넣어주는 CUDA API는 cudaGetDeviceProperties()함수이며, 원형은 다음과 같음\n",
        "```\n",
        "cudaError_t cudaGetDeviceProperties (cudaDeviceProp* prop, int deviceID)\n",
        "```\n",
        " - cudaDeviceProp* prop : 얻어온 정보를 저장할 cudaDeviceProp 구조체 변수의 주소\n",
        " - int deviceID : 정보를 얻어올 GPU번호로 0번부터 시작함\n",
        "- 시스템 내부 GPU 개수는 cudaGetDeviceCount()함수를 통해 알 수 있으며 원형은 다음과 같음\n",
        "\n",
        "\n",
        "```\n",
        "cudaError_t cudaGetDeviceCount(int *count)\n",
        "```\n",
        " - int* count 는 GPU 개수를 저장할 int형 변수의 주소\n",
        "\n",
        "- 시스템 내 GPU 정보를 출력하는 프로그램 예시"
      ],
      "metadata": {
        "id": "Ph0lB5CCn-hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"helper_cuda.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define _1MB (1024*1024)\n",
        "\n",
        "void main(void){\n",
        "    int ngpus;\n",
        "    cudaGetDeviceCount(&ngpus); //시스템 내 GPU 개수 ngpus변수에 저장\n",
        "    for(int i=0; i<ngpus; i++){\n",
        "        cudaDeviceProp devProp; //cudaDeviceProp 구조체 변수 devProp 선언\n",
        "\n",
        "        cudaGetDeviceProperties(&devProp, i); // i번째 GPU 정보를 얻어 devProp에 저장\n",
        "\n",
        "        printf(\"Device %d: %s\\n\", i, devProp.name);  //이름\n",
        "        printf(\"\\tCompute capability: %d.%d\\n\", devProp.major, devProp.minor); //compute capability 주 버전\n",
        "        printf(\"\\tThe number of streaming multiprocessors: %d\\n\", devProp.multiProcessorCount); //SM 수\n",
        "        printf(\"\\tThe number of CUDA cores: %d\\n\", _ConvertSMVer2Cores(devProp.major, devProp.minor) * devProp.multiProcessorCount); //CUDA 코어 수\n",
        "        printf(\"\\tGolbal memory size: %.2f MB\", (float)devProp.totalGlobalMem/ _1MB); //디바이스 메모리 크기\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nRIhyLy8yGE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}